# Data Warehouse with Airflow

This is an upgrade using Airflow as a workflow management tool based on [my previous project](https://github.com/wctjerry/Data-Warehouse). 

## Project Description

An imaginary music streaming company, Sparkify, has decided that it is time to introduce more automation and monitoring to their data warehouse ETL pipelines and come to the conclusion that the best tool to achieve this is Apache Airflow.

I will work as a data engineer to create high grade data pipelines that are dynamic and built from reusable tasks, can be monitored, and allow easy backfills. They have also noted that the data quality plays a big part when analyses are executed on top the data warehouse and want to run tests against their datasets after the ETL steps have been executed to catch any discrepancies in the datasets.

## Roadmap

![workflow](images/workflow.png)

This project consists of three major steps:

1. Extract from a source (in S3) to a destination (in Redshift) for staging
2. Transform the data structure and load into analytics tables
3. Execute some quality checks and evaluate the data quality

### Step One: Staging

It often requires less code to perform ETL without staging data from sources, but pipelines without staging will have a few pitfalls:

* Debug. It's difficult to debug when some unexpected quality issues happen. Without staging tables, you have to rerun the whole pipeline, which is super time-consuming.
* Performance. Sometimes you will have multiple data sources, not only AWS S3, but also multiple transformation between different databases, or even calling external APIs. Re-run pipelines (and always including backfills) tends to have performance issues.
* Data availability. Sometimes you don't always have access to source data (access control or performance consideration). Without staging tables, it becomes impossible to re-execute your data pipelines.

So based on above discussions, our first step is to stage our source data. 

#### Datasets

The source data resides in S3 and needs to be processed in Sparkify's data warehouse in Amazon Redshift. The source datasets consist of JSON logs that tell about user activity in the application and JSON metadata about the songs the users listen to.

- Log data: `s3://udacity-dend/log_data` (Log data json path: `s3://udacity-dend/log_json_path.json`)
- Song data: `s3://udacity-dend/song_data`

##### Log Dataset

The second dataset consists of log files in JSON format. It is fake and generated by an event simulator based on above song dataset and some random configuration settings.

The log files in the dataset are partitioned by year and month. For example, here are filepaths to two files in this dataset.

```
log_data/2018/11/2018-11-12-events.json
log_data/2018/11/2018-11-13-events.json
```

This is a great signal that the pipeline could apply *incremental insert* when loading into the staging table: In every logical date, we only need to check and copy a single json file by the path of `log_data/{{execution_year}}/{{execution_month}}/{{execution_date}}-events.json`. There is no need to check other files or folders, which is very performance effective.

Below is an example of what the data looks like in a log file:

![log-event](images/log-data.png)



##### Song Dataset

The song dataset is a subset of real data from the Million Song Dataset. Each file is in JSON format and contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID. For example, here are filepaths to two files in this dataset.

```
song_data/A/B/C/TRABCEI128F424C983.json
song_data/A/A/B/TRAABJL12903CDCF1A.json
```

And below is an example of what a single song file, TRAABJL12903CDCF1A.json, looks like.

```
{"num_songs": 1, "artist_id": "ARJIE2Y1187B994AB7", "artist_latitude": null, "artist_longitude": null, "artist_location": "", "artist_name": "Line Renaud", "song_id": "SOUPIRU12A6D4FA1E1", "title": "Der Kleine Dompfaff", "duration": 152.92036, "year": 0}
```

In most production-grade applications, the song records will have a few timestamps to identify at least created_date/modified_date, which is a good signal to apply incremental insert when loading songs into a staging table. In every logical date,  (how about deleted records??)

```
If execution_date = created_date:
		insert the record
elif execution_date = modified_date:
		update the record
		
```

As such, in a production_grade application, we normally don't need to check every single records. Only need to check those newly changed. However, in this song dataset, we don't have this privilege as it has no timestamps to identify those changes, so I will go with a truncate-insert approach. 





transofrm

* fact table - incremental, large and source is by date
* Dimension - ideally insert/update, but here replace (user can update?)

Data quality 

- completeness. Nun of average events, nun of average users daily, all songs exist, all user information exists
- Consistency. Same source, so not a problem here
- Validity. Time = execution date. Song duration greater than one min